{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d046124d-a096-4cea-9af0-de299b441d54",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6fd761-442c-4fbe-8c45-e2a14afe8e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6b8aca-8d61-4f93-a0c0-d4f2d192fa00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read\n",
    "df = pd.read_csv(\"../data/simplemaps_worldcities_basicv1.76/worldcities.csv\")\n",
    "\n",
    "# Cities > 1,000,000 population\n",
    "df_pop = df.query(\"population > 1000000\")\n",
    "\n",
    "# Drop duplicates (take more populous entry)\n",
    "df_pop = df_pop.sort_values(by=\"population\", ascending=False).drop_duplicates(subset=[\"city_ascii\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8711936-ce89-4a96-8988-870c6e662642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure largest city in each country is added to database\n",
    "df_country_maxs = df.loc[df.dropna(subset='population').groupby(\"country\")[\"population\"].idxmax()].reset_index(drop=True)\n",
    "\n",
    "df_pop = pd.concat([df_pop, df_country_maxs]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35fbab5-757c-439e-bb18-3f964e338c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Include some others\n",
    "fun_cities = [\n",
    "    (\"Glasgow\", \"United Kingdom\"),\n",
    "    (\"Edinburgh\", \"United Kingdom\"),\n",
    "    (\"Dundee\", \"United Kingdom\"),\n",
    "    (\"Aberdeen\", \"United Kingdom\"),\n",
    "    (\"Inverness\", \"United Kingdom\"),\n",
    "    (\"Elgin\", \"United Kingdom\"),\n",
    "    (\"Heidelberg\", \"Germany\"),\n",
    "    (\"Dortmund\", \"Germany\"),\n",
    "    (\"Urbana\", \"United States\"),\n",
    "    (\"Champaign\", \"United States\"),\n",
    "    (\"Carbondale\", \"United States\"),\n",
    "    (\"Ithaca\", \"United States\"),\n",
    "]\n",
    "\n",
    "for city, country in fun_cities:\n",
    "    if city in [\"Urbana\", \"Carbondale\"]:\n",
    "        city_info = df[(df.city_ascii == city) & (df.country == country) & (df.admin_name == \"Illinois\")]\n",
    "    else:\n",
    "        city_info = df[(df.city_ascii == city) & (df.country == country)]\n",
    "\n",
    "    df_pop = pd.concat([df_pop, city_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dc5a805-bf87-42e8-8044-8d953b15ae05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out\n",
    "df_out = df_pop[[\"city_ascii\", \"country\", \"lat\", \"lng\"]]\n",
    "\n",
    "df_out.to_csv(\"../data/simplemaps_worldcities_basicv1.76/worldcities_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be6150-6110-4e86-8865-d3145ad9582e",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2046aa35-59c6-416d-98d6-91ed52d81208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c85bdee1-073f-47e2-93f1-12a0adfcc97c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "out_path = \"/storage/home/dcl5300/work/uc_dashboard_example/data/csv/\"  # where to store intermediate data files to speed up plotting\n",
    "\n",
    "nex_in = \"/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/\"  # location of NEX-GDDP metrics\n",
    "cil_in = \"/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/cil-gdpcir/\"  # location of CIL-GDPCIR metrics\n",
    "isi_in = \"/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/isimip3b/regridded/conservative/\"  # location of *regridded* ISIMIP metrics\n",
    "cbp_in = \"/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/\"  # location of carbonplan metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09543d5c-d695-4559-889d-fb1caef28713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "#######################################\n",
    "############## NEX & CIL ##############\n",
    "#######################################\n",
    "\n",
    "ssp_all = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "ssp_missing370 = [\"ssp126\", \"ssp245\", \"ssp585\"]\n",
    "ssp_missing585 = [\"ssp126\", \"ssp245\", \"ssp370\"]\n",
    "\n",
    "nex_ssp_dict = {\n",
    "    \"ACCESS-ESM1-5\": ssp_all,\n",
    "    \"BCC-CSM2-MR\": ssp_all,\n",
    "    \"CanESM5\": ssp_all,\n",
    "    \"CMCC-ESM2\": ssp_all,\n",
    "    \"CNRM-CM6-1\": ssp_all,\n",
    "    \"CNRM-ESM2-1\": ssp_all,\n",
    "    \"EC-Earth3\": ssp_all,\n",
    "    \"EC-Earth3-Veg-LR\": ssp_all,\n",
    "    \"GFDL-ESM4\": ssp_all,\n",
    "    \"HadGEM3-GC31-LL\": ssp_missing370,\n",
    "    \"INM-CM4-8\": ssp_all,\n",
    "    \"INM-CM5-0\": ssp_all,\n",
    "    \"IPSL-CM6A-LR\": ssp_all,\n",
    "    \"MIROC-ES2L\": ssp_all,\n",
    "    \"MIROC6\": ssp_all,\n",
    "    \"MPI-ESM1-2-HR\": ssp_all,\n",
    "    \"MPI-ESM1-2-LR\": ssp_all,\n",
    "    \"MRI-ESM2-0\": ssp_all,\n",
    "    \"NESM3\": ssp_missing370,\n",
    "    \"NorESM2-LM\": ssp_all,\n",
    "    \"NorESM2-MM\": ssp_all,\n",
    "    \"UKESM1-0-LL\": ssp_all,\n",
    "}\n",
    "\n",
    "cil_ssp_dict = {\n",
    "    \"ACCESS-ESM1-5\": ssp_missing585,\n",
    "    \"BCC-CSM2-MR\": ssp_all,\n",
    "    \"CanESM5\": ssp_all,\n",
    "    \"CMCC-ESM2\": ssp_all,\n",
    "    \"EC-Earth3\": ssp_all,\n",
    "    \"EC-Earth3-Veg-LR\": ssp_all,\n",
    "    \"GFDL-ESM4\": ssp_all,\n",
    "    \"HadGEM3-GC31-LL\": ssp_missing370,\n",
    "    \"INM-CM4-8\": ssp_all,\n",
    "    \"INM-CM5-0\": ssp_all,\n",
    "    \"MIROC-ES2L\": ssp_all,\n",
    "    \"MIROC6\": ssp_all,\n",
    "    \"MPI-ESM1-2-LR\": ssp_all,\n",
    "    \"NESM3\": ssp_missing370,\n",
    "    \"NorESM2-LM\": ssp_all,\n",
    "    \"NorESM2-MM\": ssp_all,\n",
    "    \"UKESM1-0-LL\": ssp_all,\n",
    "}\n",
    "\n",
    "#######################################\n",
    "############### ISIMIP3b ##############\n",
    "#######################################\n",
    "\n",
    "ssp_missing245 = [\"ssp126\", \"ssp370\", \"ssp585\"]\n",
    "\n",
    "isimip_ssp_dict = {\n",
    "    \"CanESM5\": ssp_missing245,\n",
    "    \"CNRM-CM6-1\": ssp_missing245,\n",
    "    \"CNRM-ESM2-1\": ssp_missing245,\n",
    "    \"EC-Earth3\": ssp_missing245,\n",
    "    \"GFDL-ESM4\": ssp_all,\n",
    "    \"IPSL-CM6A-LR\": ssp_all,\n",
    "    \"MIROC6\": ssp_missing245,\n",
    "    \"MPI-ESM1-2-HR\": ssp_all,\n",
    "    \"MRI-ESM2-0\": ssp_all,\n",
    "    \"UKESM1-0-LL\": ssp_all,\n",
    "}\n",
    "\n",
    "########################################\n",
    "############## carbonplan ##############\n",
    "########################################\n",
    "\n",
    "# GARD-SV\n",
    "ssp_missing126 = [\"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "var_all = [\"tasmin\", \"tasmax\", \"pr\"]\n",
    "var_missing_pr = [\"tasmin\", \"tasmax\"]\n",
    "\n",
    "gardsv_ssp_dict = {\n",
    "    \"BCC-CSM2-MR\": ssp_missing126,\n",
    "    \"CanESM5\": ssp_missing126,\n",
    "    \"MIROC6\": ssp_missing126,\n",
    "    \"MPI-ESM1-2-HR\": ssp_missing126,\n",
    "}\n",
    "\n",
    "gardsv_var_dict = {\n",
    "    \"BCC-CSM2-MR\": var_missing_pr,\n",
    "    \"CanESM5\": var_all,\n",
    "    \"MIROC6\": var_missing_pr,\n",
    "    \"MPI-ESM1-2-HR\": var_all,\n",
    "}\n",
    "\n",
    "# DeepSD-BC\n",
    "deepsdbc_dict = {\n",
    "    \"CanESM5\": {\"ssp245\": var_all, \"ssp370\": var_all, \"ssp585\": var_all},\n",
    "    \"MRI-ESM2-0\": {\"ssp245\": var_all, \"ssp370\": var_all, \"ssp585\": var_missing_pr},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7499205-fc64-413b-8dac-86335e5f0943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models dict\n",
    "nex_models = list(nex_ssp_dict.keys())\n",
    "cil_models = list(cil_ssp_dict.keys())\n",
    "isi_models = list(isimip_ssp_dict.keys())\n",
    "cbp_gard_models = list(gardsv_ssp_dict.keys())\n",
    "cbp_gard_precip_models = [model for model in cbp_gard_models if \"pr\" in gardsv_var_dict[model]]\n",
    "cbp_deep_models = list(deepsdbc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a9c524-4b66-47e3-b1f6-087cd9c079fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Get raw timeseries for given lat/lon and write to file\n",
    "# using dask for speedup\n",
    "#########################################################\n",
    "def get_timeseries_latlon(lat, lon, metric, var_id, out_str):\n",
    "    ## Subfunction to read and process each ensemble\n",
    "    def read_and_process(ensemble, path_in, model, metric, submetric, submetric_var, lat, lon):\n",
    "        if metric in [\"hot\"]:\n",
    "            model_str = (\n",
    "                model\n",
    "                + \"_\"\n",
    "                + var_id.replace(\"_count\", \"\").replace(\"_streak\", \"\").replace(\"q99\", \"\").replace(\"rp10\", \"\")[:-5]\n",
    "            )\n",
    "        else:\n",
    "            model_str = model\n",
    "\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in [\"NEX\", \"ISIMIP\", \"GARD-SV\"]:\n",
    "            ds = xr.open_dataset(path_in + metric + \"/\" + model_str + \".nc\")\n",
    "        elif ensemble in [\"CIL\", \"DeepSD-BC\"]:\n",
    "            ds = xr.open_dataset(path_in + metric + \"/\" + model, engine=\"zarr\")\n",
    "\n",
    "        # Select submetric\n",
    "        ds = ds[submetric]\n",
    "\n",
    "        # Select lat\n",
    "        ds = ds.sel(lat=lat, method=\"nearest\")\n",
    "\n",
    "        # Common preprocessing\n",
    "        ds[\"time\"] = ds.indexes[\"time\"].year\n",
    "        if ds.lon.max() > 180:\n",
    "            ds[\"lon\"] = np.where(ds[\"lon\"] > 180, ds[\"lon\"] - 360, ds[\"lon\"])\n",
    "            ds = ds.sortby(\"lon\")\n",
    "\n",
    "        # Select lon\n",
    "        ds = ds.sel(lon=lon, method=\"nearest\")\n",
    "\n",
    "        # Construct dataframe\n",
    "        df_tmp = ds.to_dataframe().drop(columns=[\"lat\", \"lon\"]).reset_index()\n",
    "        df_tmp[\"ensemble\"] = ensemble\n",
    "        df_tmp[\"model\"] = model\n",
    "\n",
    "        # DeepSD-BC still has member_id\n",
    "        if \"member_id\" in df_tmp.columns:\n",
    "            df_tmp = df_tmp.drop(columns=\"member_id\")\n",
    "\n",
    "        return df_tmp\n",
    "\n",
    "    ## Read all combinations\n",
    "    delayed_df = []\n",
    "    # df = pd.DataFrame(columns = ['time', 'model', 'ensemble', 'ssp', var_id])\n",
    "\n",
    "    # NEX\n",
    "    for model in nex_models:\n",
    "        df_tmp = dask.delayed(read_and_process)(\"NEX\", nex_in, model, metric, var_id, False, lat, lon)\n",
    "        delayed_df.append(df_tmp)\n",
    "\n",
    "    # CIL\n",
    "    for model in cil_models:\n",
    "        df_tmp = dask.delayed(read_and_process)(\"CIL\", cil_in, model, metric, var_id, False, lat, lon)\n",
    "        delayed_df.append(df_tmp)\n",
    "\n",
    "    # ISIMIP\n",
    "    for model in isi_models:\n",
    "        df_tmp = dask.delayed(read_and_process)(\"ISIMIP\", isi_in, model, metric, var_id, False, lat, lon)\n",
    "        delayed_df.append(df_tmp)\n",
    "\n",
    "    # GARD-SV\n",
    "    if metric in [\"wet\", \"dry\", \"hotdry\"] or var_id == \"pr\":\n",
    "        model_list = cbp_gard_precip_models\n",
    "    else:\n",
    "        model_list = cbp_gard_models\n",
    "    for model in model_list:\n",
    "        df_tmp = dask.delayed(read_and_process)(\n",
    "            \"GARD-SV\",\n",
    "            cbp_in + \"regridded/conservative/GARD-SV/\",\n",
    "            model,\n",
    "            metric,\n",
    "            var_id,\n",
    "            False,\n",
    "            lat,\n",
    "            lon,\n",
    "        )\n",
    "        delayed_df.append(df_tmp)\n",
    "\n",
    "    # DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        df_tmp = dask.delayed(read_and_process)(\n",
    "            \"DeepSD-BC\",\n",
    "            cbp_in + \"native_grid/DeepSD-BC/\",\n",
    "            model,\n",
    "            metric,\n",
    "            var_id,\n",
    "            False,\n",
    "            lat,\n",
    "            lon,\n",
    "        )\n",
    "        delayed_df.append(df_tmp)\n",
    "\n",
    "    # Compute\n",
    "    if not os.path.isfile(out_path + out_str + \".csv\"):\n",
    "        df = dask.compute(*delayed_df)\n",
    "\n",
    "        # Store\n",
    "        pd.concat(df).to_csv(out_path + out_str + \".csv\", index=False)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85992a78-3a94-4b87-8d72-6f290fb1133d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_jobqueue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Dask\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_jobqueue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PBSCluster\n\u001b[1;32m      6\u001b[0m cluster \u001b[38;5;241m=\u001b[39m PBSCluster(\n\u001b[1;32m      7\u001b[0m     cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m15GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     walltime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m00:30:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m cluster\u001b[38;5;241m.\u001b[39mscale(jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# ask for jobs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_jobqueue'"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Dask\n",
    "###################\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory=\"15GB\",\n",
    "    resource_spec=\"pmem=15GB\",\n",
    "    worker_extra_args=[\"#PBS -l feature=rhel7\"],\n",
    "    walltime=\"00:30:00\",\n",
    ")\n",
    "\n",
    "cluster.scale(jobs=20)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "babbf7c6-cc07-439d-bcc8-00758bbb9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read city list\n",
    "city_list = pd.read_csv(\"../data/simplemaps_worldcities_basicv1.76/worldcities_subset.csv\")\n",
    "\n",
    "city_list[\"city_ascii\"] = city_list[\"city_ascii\"].apply(lambda x: x.replace(\" \", \"X-X\"))\n",
    "city_list[\"country\"] = city_list[\"country\"].apply(lambda x: x.replace(\" \", \"X-X\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6917b04b-7bc0-4271-b00c-9faddaddd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.3 s, sys: 2.95 s, total: 1min 1s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##################################\n",
    "# Create and store all necessary\n",
    "# timeseries for the figures\n",
    "##################################\n",
    "## Averages\n",
    "metric = \"avg\"\n",
    "for var_id in [\"tas\", \"pr\"]:\n",
    "    for city in city_list[\"city_ascii\"]:\n",
    "        city_info = city_list[city_list.city_ascii == city]\n",
    "        lat, lon = city_info[\"lat\"].values[0], city_info[\"lng\"].values[0]\n",
    "        country = city_info[\"country\"].values[0]\n",
    "        get_timeseries_latlon(lat, lon, metric, var_id, city + \"_\" + country + \"_\" + metric + \"_\" + var_id)\n",
    "\n",
    "## 1-day Max\n",
    "metric = \"max\"\n",
    "for var_id in [\"pr\", \"tasmax\"]:\n",
    "    for city in city_list[\"city_ascii\"]:\n",
    "        city_info = city_list[city_list.city_ascii == city]\n",
    "        lat, lon = city_info[\"lat\"].values[0], city_info[\"lng\"].values[0]\n",
    "        country = city_info[\"country\"].values[0]\n",
    "        get_timeseries_latlon(lat, lon, metric, var_id, city + \"_\" + country + \"_\" + metric + \"_\" + var_id)\n",
    "\n",
    "\n",
    "## Dry days\n",
    "metric = \"dry\"\n",
    "for var_id in [\"count_lt_1\"]:\n",
    "    for city in city_list[\"city_ascii\"]:\n",
    "        city_info = city_list[city_list.city_ascii == city]\n",
    "        lat, lon = city_info[\"lat\"].values[0], city_info[\"lng\"].values[0]\n",
    "        country = city_info[\"country\"].values[0]\n",
    "        get_timeseries_latlon(lat, lon, metric, var_id, city + \"_\" + country + \"_\" + metric + \"_\" + var_id)\n",
    "\n",
    "## Extremely Hot Days\n",
    "metric = \"hot\"\n",
    "for var_id in [\"tasmax_q99gmfd_count\"]:\n",
    "    for city in city_list[\"city_ascii\"]:\n",
    "        city_info = city_list[city_list.city_ascii == city]\n",
    "        lat, lon = city_info[\"lat\"].values[0], city_info[\"lng\"].values[0]\n",
    "        country = city_info[\"country\"].values[0]\n",
    "        get_timeseries_latlon(lat, lon, metric, var_id, city + \"_\" + country + \"_\" + metric + \"_\" + var_id)\n",
    "\n",
    "## Extremely Wet Days\n",
    "metric = \"wet\"\n",
    "for var_id in [\"pr_q99gmfd_count\"]:\n",
    "    for city in city_list[\"city_ascii\"]:\n",
    "        city_info = city_list[city_list.city_ascii == city]\n",
    "        lat, lon = city_info[\"lat\"].values[0], city_info[\"lng\"].values[0]\n",
    "        country = city_info[\"country\"].values[0]\n",
    "        get_timeseries_latlon(lat, lon, metric, var_id, city + \"_\" + country + \"_\" + metric + \"_\" + var_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60ff8e-9acc-4e24-bc7a-8c390b5aa69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
